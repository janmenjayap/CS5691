Training dataset  ####################
Shape of Feature matrix: (10000, 100)
Shape of Value vector: (10000,)


########################################
Maximum Likelihood Estimated Regression
########################################

Analytical Maximum Likelihood Solution  ####################
Shape of  wML: (100,)
wML (Analytical): 
[-7.84961009e-03 -1.36715320e-02 -3.61656438e-03  2.64909160e-03
  1.88551446e-01  2.65314657e-03  9.46531786e-03  1.79809481e-01
  3.73757317e-03  4.99608944e-01  8.35836265e-03  4.29108775e-03
  1.42141179e-02  3.94232414e-03  9.36795890e-03 -1.12038274e-03
  3.35727500e-03  1.16152212e-03 -9.40884707e-03 -2.45575476e-03
 -1.17409629e-02 -1.01960612e-02  7.95771321e-03 -1.00574854e-02
  6.04882939e-03 -4.67345192e-03 -3.09091547e-03  8.14909193e-03
  1.20264599e-02 -6.82458163e-03 -8.65405539e-03  9.86273479e-04
  4.92968011e-03  5.99772461e-03 -1.34667860e-02  1.07075729e-03
  1.32745992e-02 -1.14148742e-02 -2.01056697e-02  5.85096240e-01
  4.94483247e-04 -7.86666920e-04 -2.71926574e-03 -9.54021938e-03
 -5.44161058e-03  9.80679209e-03 -6.72540624e-03 -4.45414276e-04
  6.98516508e-03  3.16138907e-02  4.51763485e-01 -8.75221380e-03
  2.55167390e-03  4.24921150e-03  2.89847927e-01  7.03723255e-03
 -1.95796946e-03  1.41523883e-02 -1.06508170e-02  7.72743903e-01
 -5.67126044e-03 -6.30026188e-04  6.50943015e-03 -4.84019165e-03
  4.63832329e-03  4.54887177e-03 -2.99475114e-03  8.38781696e-03
 -2.47558716e-03  9.00947922e-04  1.14713514e-03 -1.87641345e-03
 -1.05175760e-02 -9.31304110e-03 -1.23550002e-03  5.97797559e-01
 -4.78625013e-03 -1.13727852e-02  2.88477060e-03  8.48999776e-01
 -1.08924235e-02  2.26346489e-03 -1.38099800e-03 -6.35934691e-03
  5.83784109e-03  5.69286755e-03  5.35566859e-03 -8.20616315e-03
  1.29884015e-02 -2.30575631e-03 -1.22263765e-04  8.66629171e-03
 -4.29446300e-03  5.69510898e-03  7.55483353e-03 -9.43540843e-03
  1.82905446e-02 -1.16998887e-03 -2.61599136e-03 -8.58616114e-03] 

Tuning Learning rate for batch mode of gradient descent in maximum likelihood estimation:
Learning Rate: 5e-08, Loss: 0.19395063147037228, Norm Difference: 1.4627443028608227
Learning Rate: 1e-07, Loss: 0.15848134390210555, Norm Difference: 1.3722980065796149
Learning Rate: 5e-07, Loss: 0.05530450867885704, Norm Difference: 0.8706861317075958
Learning Rate: 1e-06, Loss: 0.041757381889203936, Norm Difference: 0.5556253780390125
Learning Rate: 3e-06, Loss: 0.04065692827650064, Norm Difference: 0.20097954639772547
Learning Rate: 3.5e-06, Loss: 0.040657556384647804, Norm Difference: 0.17224777718459325
Learning Rate: 3.75e-06, Loss: 0.04065769614442924, Norm Difference: 0.16073405073240019
Learning Rate: 4e-06, Loss: 0.0406577775250922, Norm Difference: 0.15065635649158507
Learning Rate: 5e-06, Loss: 593275867149933.1, Norm Difference: 158874.5139436548
Learning Rate: 1e-05, Loss: nan, Norm Difference: nan
Learning Rate: 5e-05, Loss: nan, Norm Difference: nan

Optimal Learning Rate: 3e-06
Epoch: 1, Learning Rate: 3e-06, Loss: 1.494283789272762, Norm Difference: 1.5682563851542435
Epoch: 2, Learning Rate: 3e-06, Loss: 0.5569146758082525, Norm Difference: 1.5486730638626829
Epoch: 3, Learning Rate: 3e-06, Loss: 0.3161167557223516, Norm Difference: 1.537968968040264
Epoch: 4, Learning Rate: 3e-06, Loss: 0.2531954583346647, Norm Difference: 1.5295855679670676
Epoch: 5, Learning Rate: 3e-06, Loss: 0.23570748336929273, Norm Difference: 1.521826950361447
Epoch: 6, Learning Rate: 3e-06, Loss: 0.22983469399006895, Norm Difference: 1.514256929316754
Epoch: 7, Learning Rate: 3e-06, Loss: 0.2269418434534549, Norm Difference: 1.5067633749530183
Epoch: 8, Learning Rate: 3e-06, Loss: 0.22482387016605399, Norm Difference: 1.499317366429609
Epoch: 9, Learning Rate: 3e-06, Loss: 0.2229175935178053, Norm Difference: 1.4919113392241399
Epoch: 10, Learning Rate: 3e-06, Loss: 0.22107907122465362, Norm Difference: 1.4845432099779745
Epoch: 100, Learning Rate: 3e-06, Loss: 0.11375611803194272, Norm Difference: 0.952539108057926
Epoch: 200, Learning Rate: 3e-06, Loss: 0.06730294885358541, Norm Difference: 0.5842401006803338
Epoch: 300, Learning Rate: 3e-06, Loss: 0.05007454798564643, Norm Difference: 0.35989689416946796
Epoch: 400, Learning Rate: 3e-06, Loss: 0.04362778135945991, Norm Difference: 0.22262801408311048
Epoch: 500, Learning Rate: 3e-06, Loss: 0.04119431950419656, Norm Difference: 0.13826838772190606
Epoch: 600, Learning Rate: 3e-06, Loss: 0.04026795237694386, Norm Difference: 0.08620372746862205
Epoch: 700, Learning Rate: 3e-06, Loss: 0.03991241531881283, Norm Difference: 0.0539389950175352
Epoch: 800, Learning Rate: 3e-06, Loss: 0.03977489123251595, Norm Difference: 0.03386602596120869
Epoch: 900, Learning Rate: 3e-06, Loss: 0.03972129912167598, Norm Difference: 0.021331458528820547
Epoch: 1000, Learning Rate: 3e-06, Loss: 0.039700267189162114, Norm Difference: 0.013476691875044787
Epoch: 1100, Learning Rate: 3e-06, Loss: 0.0396919583920103, Norm Difference: 0.008538203503914212
Epoch: 1200, Learning Rate: 3e-06, Loss: 0.03968865543861861, Norm Difference: 0.005423600482543236
Epoch: 1300, Learning Rate: 3e-06, Loss: 0.03968733475136266, Norm Difference: 0.003453572498684639
Epoch: 1400, Learning Rate: 3e-06, Loss: 0.03968680378823259, Norm Difference: 0.0022041210931275303
Epoch: 1500, Learning Rate: 3e-06, Loss: 0.03968658923429581, Norm Difference: 0.0014096761563591814
Epoch: 1600, Learning Rate: 3e-06, Loss: 0.03968650212476267, Norm Difference: 0.0009033500890822006
Epoch: 1700, Learning Rate: 3e-06, Loss: 0.039686466601706744, Norm Difference: 0.0005799440726022838
Epoch: 1800, Learning Rate: 3e-06, Loss: 0.039686452055893884, Norm Difference: 0.00037295318240660634
Epoch: 1900, Learning Rate: 3e-06, Loss: 0.03968644607693594, Norm Difference: 0.00024022034819677548
Epoch: 2000, Learning Rate: 3e-06, Loss: 0.03968644361056438, Norm Difference: 0.0001549549816364392
CPU time taken for training: 258.08565177200035 seconds

wML (Batch Mode of Gradient Descent):
[-7.84539365e-03 -1.36653828e-02 -3.63298110e-03  2.64365096e-03
  1.88524882e-01  2.65731677e-03  9.45359284e-03  1.79784909e-01
  3.75535448e-03  4.99583072e-01  8.35795835e-03  4.29792461e-03
  1.42193402e-02  3.94222067e-03  9.37473153e-03 -1.12267377e-03
  3.37735342e-03  1.17917801e-03 -9.39706620e-03 -2.43601722e-03
 -1.17391261e-02 -1.01873569e-02  7.95484727e-03 -1.00581940e-02
  6.05164368e-03 -4.66294950e-03 -3.09169489e-03  8.14556026e-03
  1.20210933e-02 -6.81929831e-03 -8.65427504e-03  9.74490205e-04
  4.95048449e-03  5.99456300e-03 -1.34575868e-02  1.06592184e-03
  1.32835638e-02 -1.14008260e-02 -2.00998058e-02  5.85066256e-01
  4.85826764e-04 -7.79518354e-04 -2.71475018e-03 -9.54095595e-03
 -5.43061744e-03  9.80393202e-03 -6.74232445e-03 -4.56127487e-04
  6.98228584e-03  3.16126382e-02  4.51745833e-01 -8.75962773e-03
  2.55888711e-03  4.24288916e-03  2.89829752e-01  7.02384480e-03
 -1.95118148e-03  1.41497186e-02 -1.06379062e-02  7.72681171e-01
 -5.66629496e-03 -6.07290727e-04  6.53024292e-03 -4.82656350e-03
  4.64013426e-03  4.54344467e-03 -2.99114626e-03  8.39450265e-03
 -2.46081254e-03  8.90955730e-04  1.15962949e-03 -1.87324018e-03
 -1.05100582e-02 -9.30826494e-03 -1.20817256e-03  5.97769067e-01
 -4.76785550e-03 -1.13499768e-02  2.89944327e-03  8.48927303e-01
 -1.08913594e-02  2.27590557e-03 -1.37140208e-03 -6.37571973e-03
  5.84057271e-03  5.67938275e-03  5.35091238e-03 -8.20744013e-03
  1.29918968e-02 -2.29924207e-03 -1.34644651e-04  8.65576275e-03
 -4.30407621e-03  5.68131266e-03  7.55447763e-03 -9.41323055e-03
  1.82989715e-02 -1.15681823e-03 -2.61377470e-03 -8.57725574e-03] 

Tuning Learning rate for stochastic mode (batch size: 100) of gradient descent in maximum likelihood estimation:
Learning Rate: 1e-07, Loss: 0.8994638760798525, Norm Difference: 1.5846300965505922
Learning Rate: 5e-07, Loss: 0.23397895073216834, Norm Difference: 1.5538809128966924
Learning Rate: 1e-06, Loss: 0.22751407404846988, Norm Difference: 1.5380611545150864
Learning Rate: 5e-06, Loss: 0.18422999172952143, Norm Difference: 1.4392549980802793
Learning Rate: 1e-05, Loss: 0.14385309923607614, Norm Difference: 1.3299107960558543
Learning Rate: 5e-05, Loss: 0.04856049475951542, Norm Difference: 0.7685518262084704
Learning Rate: 0.0001, Loss: 0.041048883594372816, Norm Difference: 0.46283584383044973
Learning Rate: 0.0002, Loss: 0.041220540004069835, Norm Difference: 0.25015194023462045
Learning Rate: 0.00025, Loss: 0.04110951158927316, Norm Difference: 0.20702385997578776
Learning Rate: 0.0005, Loss: nan, Norm Difference: nan
Learning Rate: 0.001, Loss: nan, Norm Difference: nan
Learning Rate: 0.005, Loss: nan, Norm Difference: nan
Learning Rate: 0.01, Loss: nan, Norm Difference: nan


Optimal Learning Rate: 0.0001
Epoch: 1, Learning Rate: 0.0001, Loss: 1.4734813002849492, Norm Difference: 1.5752800771900621
Epoch: 2, Learning Rate: 0.0001, Loss: 0.5443445923409255, Norm Difference: 1.56107005922407
Epoch: 3, Learning Rate: 0.0001, Loss: 0.2964067498247763, Norm Difference: 1.5543309449803833
Epoch: 4, Learning Rate: 0.0001, Loss: 0.2626455258991784, Norm Difference: 1.5516946839551804
Epoch: 5, Learning Rate: 0.0001, Loss: 0.2460701123274935, Norm Difference: 1.5489809646598627
Epoch: 6, Learning Rate: 0.0001, Loss: 0.2371207613760741, Norm Difference: 1.5467512360393414
Epoch: 7, Learning Rate: 0.0001, Loss: 0.2361124869646109, Norm Difference: 1.5441139440774208
Epoch: 8, Learning Rate: 0.0001, Loss: 0.23560862458354623, Norm Difference: 1.5417949158104878
Epoch: 9, Learning Rate: 0.0001, Loss: 0.23486407508377252, Norm Difference: 1.5394025597473595
Epoch: 10, Learning Rate: 0.0001, Loss: 0.23480729353344945, Norm Difference: 1.5369680326107424
Epoch: 100, Learning Rate: 0.0001, Loss: 0.18650616810485926, Norm Difference: 1.3282690357793019
Epoch: 200, Learning Rate: 0.0001, Loss: 0.14505993299446915, Norm Difference: 1.126592074918666
Epoch: 300, Learning Rate: 0.0001, Loss: 0.11472852550609368, Norm Difference: 0.9569425587142864
Epoch: 400, Learning Rate: 0.0001, Loss: 0.09372666450173091, Norm Difference: 0.8143236350515383
Epoch: 500, Learning Rate: 0.0001, Loss: 0.07874530992405043, Norm Difference: 0.6939049120150166
Epoch: 600, Learning Rate: 0.0001, Loss: 0.06788525867698747, Norm Difference: 0.5893997418204474
Epoch: 700, Learning Rate: 0.0001, Loss: 0.06016451593940354, Norm Difference: 0.5016901136367977
Epoch: 800, Learning Rate: 0.0001, Loss: 0.05453732596067655, Norm Difference: 0.42743912627092606
Epoch: 900, Learning Rate: 0.0001, Loss: 0.05040027737001307, Norm Difference: 0.3649768401557581
Epoch: 1000, Learning Rate: 0.0001, Loss: 0.047912186962846284, Norm Difference: 0.31023458627266337
Epoch: 1100, Learning Rate: 0.0001, Loss: 0.04528094076813827, Norm Difference: 0.26473684689663834
Epoch: 1200, Learning Rate: 0.0001, Loss: 0.04387133653391217, Norm Difference: 0.22666729615838968
Epoch: 1300, Learning Rate: 0.0001, Loss: 0.04269735760527843, Norm Difference: 0.19427263357157237
Epoch: 1400, Learning Rate: 0.0001, Loss: 0.042405574887881534, Norm Difference: 0.1657227984097118
Epoch: 1500, Learning Rate: 0.0001, Loss: 0.0414246923110989, Norm Difference: 0.1431008390665403
Epoch: 1600, Learning Rate: 0.0001, Loss: 0.04100214125355756, Norm Difference: 0.12315120983898097
Epoch: 1700, Learning Rate: 0.0001, Loss: 0.04085579413720559, Norm Difference: 0.10604612133047295
Epoch: 1800, Learning Rate: 0.0001, Loss: 0.04036616339168637, Norm Difference: 0.09032297209273207
Epoch: 1900, Learning Rate: 0.0001, Loss: 0.040193895877247775, Norm Difference: 0.07675812060348772
Epoch: 2000, Learning Rate: 0.0001, Loss: 0.040405088090579896, Norm Difference: 0.06792459863438657
CPU time taken for training: 152.46139128399955 seconds
wML (Stochastic Mode of Gradient Descent w/- batch size: 100):
[-5.71261437e-03 -1.31695807e-02 -3.40528413e-03  6.05829576e-04
  1.78090283e-01  2.27535380e-03  8.90394793e-03  1.75446232e-01
  6.02301846e-03  4.86973102e-01  8.02025258e-03  5.35557364e-03
  1.45311899e-02  6.20673476e-03  9.73993983e-03 -1.95411580e-03
  7.74716907e-03  2.72713252e-03 -7.74164010e-03  3.40531377e-03
 -1.14831214e-02 -9.60705061e-03  7.47065514e-03 -1.15227800e-02
  6.29392803e-03 -2.10256975e-03 -5.62851538e-03  8.29779488e-03
  1.24139209e-02 -4.42146656e-03 -7.38954912e-03  4.17557753e-03
  1.46980484e-02  4.89762394e-03 -1.14113598e-02 -8.46729175e-05
  1.56812078e-02 -6.48502719e-03 -1.86613697e-02  5.65396419e-01
  1.86093923e-03  6.93532542e-06  3.75596123e-03 -5.83269419e-03
 -3.65175282e-03  1.24089593e-02 -7.73107028e-03 -4.88921055e-03
  7.93380944e-03  2.94347828e-02  4.35848906e-01 -9.92631739e-03
  4.12262123e-03  8.02006567e-03  2.76760445e-01  4.33551906e-03
 -6.26857536e-04  1.15300643e-02 -9.89960579e-03  7.42063368e-01
 -3.57000857e-03  5.48104906e-03  1.18801185e-02 -1.74449934e-03
  8.26377487e-03  5.58002951e-03 -2.37717425e-03  8.41227826e-03
  1.45309574e-03 -1.92269417e-03  5.05027456e-03 -1.11783165e-03
 -6.26560010e-03 -8.58050226e-03  5.54396012e-03  5.77814616e-01
 -2.21280442e-03 -1.18957411e-03  4.89422313e-03  8.11869295e-01
 -8.97720955e-03  2.44778843e-03  1.42179062e-03 -5.93595992e-03
  6.95367110e-03  5.29493497e-03  6.20491645e-03 -4.53815040e-03
  1.23224664e-02 -2.80750861e-03 -7.11790152e-04  4.69550046e-03
 -4.63071453e-03  4.33224722e-03  9.01738796e-03 -5.00645079e-03
  1.76671913e-02  3.70624758e-03 -1.09329332e-03 -5.68583821e-03] 


########################################
Ridge Regression
########################################

Tuning Lambda for Ridge Regression  ####################
Lambda: 1e-05, Loss: 0.04065788932428073
Lambda: 1.2618568830660211e-05, Loss: 0.04065788931926496
Lambda: 1.5922827933410938e-05, Loss: 0.04065788931293574
Lambda: 2.0092330025650458e-05, Loss: 0.04065788930494953
Lambda: 2.5353644939701114e-05, Loss: 0.04065788929487174
Lambda: 3.199267137797385e-05, Loss: 0.04065788928215517
Lambda: 4.037017258596558e-05, Loss: 0.04065788926610873
Lambda: 5.0941380148163754e-05, Loss: 0.04065788924586058
Lambda: 6.428073117284319e-05, Loss: 0.040657889220310446
Lambda: 8.111308307896872e-05, Loss: 0.04065788918806984
Lambda: 0.00010235310218990269, Loss: 0.04065788914738745
Lambda: 0.0001291549665014884, Loss: 0.04065788909605254
Lambda: 0.00016297508346206434, Loss: 0.04065788903127644
Lambda: 0.00020565123083486514, Loss: 0.040657888949539644
Lambda: 0.00025950242113997375, Loss: 0.040657888846402236
Lambda: 0.00032745491628777284, Loss: 0.04065788871626146
Lambda: 0.00041320124001153346, Loss: 0.040657888552048674
Lambda: 0.0005214008287999684, Loss: 0.04065788834484592
Lambda: 0.0006579332246575682, Loss: 0.04065788808340198
Lambda: 0.0008302175681319744, Loss: 0.04065788775352249
Lambda: 0.001047615752789665, Loss: 0.04065788733730301
Lambda: 0.0013219411484660286, Loss: 0.04065788681215856
Lambda: 0.0016681005372000592, Loss: 0.040657886149605395
Lambda: 0.00210490414451202, Loss: 0.04065788531372309
Lambda: 0.0026560877829466868, Loss: 0.0406578842592223
Lambda: 0.003351602650938841, Loss: 0.040657882929012026
Lambda: 0.0042292428743894986, Loss: 0.04065788125114415
Lambda: 0.005336699231206307, Loss: 0.040657879134976614
Lambda: 0.006734150657750821, Loss: 0.0406578764663669
Lambda: 0.008497534359086439, Loss: 0.04065787310165565
Lambda: 0.010722672220103232, Loss: 0.040657868860157785
Lambda: 0.013530477745798061, Loss: 0.04065786351482005
Lambda: 0.017073526474706904, Loss: 0.04065785678063598
Lambda: 0.021544346900318846, Loss: 0.040657848300363154
Lambda: 0.027185882427329403, Loss: 0.040657837627023116
Lambda: 0.03430469286314919, Loss: 0.0406578242026618
Lambda: 0.04328761281083057, Loss: 0.04065780733288144
Lambda: 0.05462277217684343, Loss: 0.040657786156832895
Lambda: 0.06892612104349695, Loss: 0.04065775961273206
Lambda: 0.08697490026177834, Loss: 0.040657726399738414
Lambda: 0.10974987654930568, Loss: 0.040657684938420584
Lambda: 0.13848863713938717, Loss: 0.04065763333451613
Lambda: 0.1747528400007683, Loss: 0.040657569354899586
Lambda: 0.22051307399030456, Loss: 0.04065749043177676
Lambda: 0.2782559402207126, Loss: 0.0406573937228555
Lambda: 0.3511191734215127, Loss: 0.04065727627449253
Lambda: 0.44306214575838776, Loss: 0.040657135366125564
Lambda: 0.5590810182512223, Loss: 0.04065696916486874
Lambda: 0.7054802310718645, Loss: 0.040656777900491714
Lambda: 0.8902150854450392, Loss: 0.04065656590112284
Lambda: 1.1233240329780265, Loss: 0.04065634503742214
Lambda: 1.4174741629268048, Loss: 0.04065614045209652
Lambda: 1.7886495290574351, Loss: 0.04065599997176069
Lambda: 2.2570197196339215, Loss: 0.04065600941619339
Lambda: 2.848035868435799, Loss: 0.040656317299670466
Lambda: 3.593813663804626, Loss: 0.0406571744075954
Lambda: 4.534878508128582, Loss: 0.040658996796943826
Lambda: 5.72236765935022, Loss: 0.040662465445958786
Lambda: 7.220809018385457, Loss: 0.04066868282083922
Lambda: 9.111627561154886, Loss: 0.040679417048612505
Lambda: 11.497569953977356, Loss: 0.04069747945105946
Lambda: 14.508287784959402, Loss: 0.040727302279313794
Lambda: 18.30738280295366, Loss: 0.04077581164292428
Lambda: 23.10129700083158, Loss: 0.04085372557398327
Lambda: 29.150530628251758, Loss: 0.040977445312589705
Lambda: 36.783797718286344, Loss: 0.041171738847778624
Lambda: 46.41588833612782, Loss: 0.041473416998925976
Lambda: 58.57020818056661, Loss: 0.04193613281553476
Lambda: 73.90722033525775, Loss: 0.042636230782618446
Lambda: 93.26033468832199, Loss: 0.043679152053277645
Lambda: 117.68119524349991, Loss: 0.04520519804501143
Lambda: 148.49682622544634, Loss: 0.04739248626569551
Lambda: 187.3817422860383, Loss: 0.05045392322001374
Lambda: 236.4489412645407, Loss: 0.05462451384682053
Lambda: 298.364724028334, Loss: 0.06013616526144735
Lambda: 376.49358067924635, Loss: 0.06718010523150045
Lambda: 475.0810162102793, Loss: 0.07586209850013537
Lambda: 599.4842503189409, Loss: 0.08616121878501226
Lambda: 756.463327554629, Loss: 0.09790595760055706
Lambda: 954.5484566618328, Loss: 0.11077898570325666
Lambda: 1204.5035402587837, Loss: 0.12435357293205887
Lambda: 1519.9110829529332, Loss: 0.1381539367450187
Lambda: 1917.910261672485, Loss: 0.15172408729351639
Lambda: 2420.1282647943835, Loss: 0.16468890056690838
Lambda: 3053.8555088334124, Loss: 0.17679691692128646
Lambda: 3853.5285937105355, Loss: 0.1879431080196215
Lambda: 4862.601580065353, Loss: 0.19817747397794655
Lambda: 6135.907273413163, Loss: 0.2077096026228499
Lambda: 7742.636826811277, Loss: 0.21692030913653584
Lambda: 9770.099572992247, Loss: 0.226390450906288
Lambda: 12328.467394420633, Loss: 0.23695514609976454
Lambda: 15556.761439304722, Loss: 0.2497891758542543
Lambda: 19630.406500402685, Loss: 0.2665254461293275
Lambda: 24770.76355991714, Loss: 0.2894011710752513
Lambda: 31257.15849688235, Loss: 0.32141359802073205
Lambda: 39442.06059437648, Loss: 0.3664470518298346
Lambda: 49770.23564332114, Loss: 0.42930732526844173
Lambda: 62802.914418342465, Loss: 0.5155758450019646
Lambda: 79248.28983539186, Loss: 0.6311913112969674
Lambda: 100000.0, Loss: 0.7817035739804765


Optimal Lambda: 1.7886495290574351
Analytical Ridge Regression Solution  ####################
Shape of wR: (100,)
wR (Analytical):
[-7.73147278e-03 -1.35408330e-02 -3.55762109e-03  2.71291974e-03
  1.88188324e-01  2.74078975e-03  9.51110034e-03  1.79478486e-01
  3.86404400e-03  4.98651035e-01  8.42309314e-03  4.37808323e-03
  1.42858676e-02  4.01722389e-03  9.45372002e-03 -1.06929162e-03
  3.48553034e-03  1.30103784e-03 -9.28271861e-03 -2.30257892e-03
 -1.16388267e-02 -1.00846229e-02  8.00905390e-03 -9.95201112e-03
  6.11939130e-03 -4.55119103e-03 -3.01457699e-03  8.21931538e-03
  1.20773142e-02 -6.69882657e-03 -8.55724081e-03  1.04780474e-03
  5.09213467e-03  6.05308273e-03 -1.33322608e-02  1.14249342e-03
  1.33635816e-02 -1.12493271e-02 -1.99807820e-02  5.83932666e-01
  5.73533411e-04 -6.79482484e-04 -2.59982401e-03 -9.43929606e-03
 -5.31082561e-03  9.84394055e-03 -6.66669530e-03 -3.75102012e-04
  7.03625004e-03  3.16284476e-02  4.50919026e-01 -8.65818248e-03
  2.65334667e-03  4.29233593e-03  2.89298360e-01  7.07157511e-03
 -1.85193391e-03  1.41877517e-02 -1.04998910e-02  7.71118879e-01
 -5.54128540e-03 -4.91090746e-04  6.64730114e-03 -4.69757265e-03
  4.70757932e-03  4.60795821e-03 -2.88696729e-03  8.49222256e-03
 -2.33109929e-03  9.30145380e-04  1.28250746e-03 -1.76721065e-03
 -1.03789942e-02 -9.18659784e-03 -1.06462598e-03  5.96627719e-01
 -4.63758491e-03 -1.11979153e-02  3.02320421e-03  8.47207250e-01
 -1.07726740e-02  2.39649699e-03 -1.25569312e-03 -6.32514232e-03
  5.91630098e-03  5.74647374e-03  5.41582379e-03 -8.09938250e-03
  1.30466059e-02 -2.21655705e-03 -5.99797996e-05  8.70263511e-03
 -4.23458983e-03  5.73789965e-03  7.63360848e-03 -9.25867889e-03
  1.83705135e-02 -1.03379484e-03 -2.51812820e-03 -8.43483339e-03] 

Tuning Learning rate for batch mode of gradient descent in ridge regression:
Learning Rate: 5e-08, Loss: 0.19395818570690057, Norm Difference: 1.4584811780422398
Learning Rate: 1e-07, Loss: 0.15850558338671858, Norm Difference: 1.3680624863819408
Learning Rate: 5e-07, Loss: 0.05541441262940945, Norm Difference: 0.8670476366944811
Learning Rate: 1e-06, Loss: 0.041816301521308136, Norm Difference: 0.5528625012125364
Learning Rate: 3e-06, Loss: 0.04065565296032657, Norm Difference: 0.19985011408069764
Learning Rate: 3.5e-06, Loss: 0.04065585364603134, Norm Difference: 0.17127780361134176
Learning Rate: 3.75e-06, Loss: 0.04065591000993029, Norm Difference: 0.15982843278288072
Learning Rate: 4e-06, Loss: 0.04065594539746376, Norm Difference: 0.14980723340303617
Learning Rate: 5e-06, Loss: 636880894476239.8, Norm Difference: 164257.35988578052
Learning Rate: 1e-05, Loss: nan, Norm Difference: nan
Learning Rate: 5e-05, Loss: nan, Norm Difference: nan

Optimal Learning Rate: 3e-06
Epoch: 1, Learning Rate: 3e-06, Loss: 1.4942837892727643, Norm Difference: 1.5648810492272138
Epoch: 2, Learning Rate: 3e-06, Loss: 0.5569552836758445, Norm Difference: 1.5452716494663412
Epoch: 3, Learning Rate: 3e-06, Loss: 0.316117036992135, Norm Difference: 1.534560630549355
Epoch: 4, Learning Rate: 3e-06, Loss: 0.25320341266751945, Norm Difference: 1.526175495877557
Epoch: 5, Learning Rate: 3e-06, Loss: 0.2357064375986436, Norm Difference: 1.5184165727108507
Epoch: 6, Learning Rate: 3e-06, Loss: 0.22983649835004138, Norm Difference: 1.5108466807050969
Epoch: 7, Learning Rate: 3e-06, Loss: 0.226941779905925, Norm Difference: 1.5033534278102048
Epoch: 8, Learning Rate: 3e-06, Loss: 0.22482476321117428, Norm Difference: 1.4959078237476147
Epoch: 9, Learning Rate: 3e-06, Loss: 0.22291817258427596, Norm Difference: 1.4885022851869625
Epoch: 10, Learning Rate: 3e-06, Loss: 0.221080043917812, Norm Difference: 1.4811347232535463
Epoch: 100, Learning Rate: 3e-06, Loss: 0.11380270912122337, Norm Difference: 0.9494132287300782
Epoch: 200, Learning Rate: 3e-06, Loss: 0.06738629741257429, Norm Difference: 0.5816838664454416
Epoch: 300, Learning Rate: 3e-06, Loss: 0.0501599587354277, Norm Difference: 0.3579292260312471
Epoch: 400, Learning Rate: 3e-06, Loss: 0.043698396876636764, Norm Difference: 0.22116813186655576
Epoch: 500, Learning Rate: 3e-06, Loss: 0.04124675462916065, Norm Difference: 0.1372112190014326
Epoch: 600, Learning Rate: 3e-06, Loss: 0.04030464097195799, Norm Difference: 0.08545099023847058
Epoch: 700, Learning Rate: 3e-06, Loss: 0.03993724766181473, Norm Difference: 0.053409511009017405
Epoch: 800, Learning Rate: 3e-06, Loss: 0.0397914248243712, Norm Difference: 0.03349693441047326
Epoch: 900, Learning Rate: 3e-06, Loss: 0.039732265146167095, Norm Difference: 0.021075934153720853
Epoch: 1000, Learning Rate: 3e-06, Loss: 0.039707593626054305, Norm Difference: 0.013300728078599692
Epoch: 1100, Learning Rate: 3e-06, Loss: 0.039696943899884686, Norm Difference: 0.008417532860285664
Epoch: 1200, Learning Rate: 3e-06, Loss: 0.03969215054998423, Norm Difference: 0.005341122861528188
Epoch: 1300, Learning Rate: 3e-06, Loss: 0.03968988691321194, Norm Difference: 0.0033973504911705946
Epoch: 1400, Learning Rate: 3e-06, Loss: 0.039688761569019654, Norm Difference: 0.002165880286869178
Epoch: 1500, Learning Rate: 3e-06, Loss: 0.03968817309759393, Norm Difference: 0.0013837125423801103
Epoch: 1600, Learning Rate: 3e-06, Loss: 0.03968785096010144, Norm Difference: 0.0008857485021768215
Epoch: 1700, Learning Rate: 3e-06, Loss: 0.03968766772019037, Norm Difference: 0.0005680264090579863
Epoch: 1800, Learning Rate: 3e-06, Loss: 0.039687560292822215, Norm Difference: 0.0003648926029622502
Epoch: 1900, Learning Rate: 3e-06, Loss: 0.03968749586666205, Norm Difference: 0.00023477351151312753
Epoch: 2000, Learning Rate: 3e-06, Loss: 0.039687456585143915, Norm Difference: 0.00015127725137629677
CPU time taken for training: 276.64734160399894 seconds

wR (Batch Mode of Gradient Descent):
[-7.72735544e-03 -1.35348295e-02 -3.57364412e-03  2.70760933e-03
  1.88162392e-01  2.74486049e-03  9.49965706e-03  1.79454497e-01
  3.88140152e-03  4.98625775e-01  8.42269858e-03  4.38475695e-03
  1.42909656e-02  4.01712354e-03  9.46033129e-03 -1.07153030e-03
  3.50512866e-03  1.31827323e-03 -9.27121947e-03 -2.28331210e-03
 -1.16370341e-02 -1.00761277e-02  8.00625596e-03 -9.95270174e-03
  6.12213726e-03 -4.54093797e-03 -3.01533776e-03  8.21586969e-03
  1.20720764e-02 -6.69366770e-03 -8.55745459e-03  1.03630529e-03
  5.11244583e-03  6.04999579e-03 -1.33232805e-02  1.13777407e-03
  1.33723333e-02 -1.12356119e-02 -1.99750589e-02  5.83903387e-01
  5.65085940e-04 -6.72504153e-04 -2.59541405e-03 -9.44001439e-03
 -5.30009346e-03  9.84114727e-03 -6.68320883e-03 -3.85558260e-04
  7.03343847e-03  3.16272242e-02  4.50901790e-01 -8.66541704e-03
  2.66038844e-03  4.28616434e-03  2.89280614e-01  7.05850751e-03
 -1.84530746e-03  1.41851450e-02 -1.04872876e-02  7.71057631e-01
 -5.53643619e-03 -4.68898383e-04  6.66761833e-03 -4.68426810e-03
  4.70934638e-03  4.60266065e-03 -2.88344739e-03  8.49875075e-03
 -2.31667540e-03  9.20389622e-04  1.29470568e-03 -1.76411114e-03
 -1.03716543e-02 -9.18193403e-03 -1.03794995e-03  5.96599899e-01
 -4.61962843e-03 -1.11756502e-02  3.03752890e-03  8.47136493e-01
 -1.07716337e-02  2.40864240e-03 -1.24632436e-03 -6.34112537e-03
  5.91896775e-03  5.73331340e-03  5.41118135e-03 -8.10062741e-03
  1.30500174e-02 -2.21019973e-03 -7.20632536e-05  8.69235806e-03
 -4.24397359e-03  5.72443400e-03  7.63326234e-03 -9.23702870e-03
  1.83787402e-02 -1.02093721e-03 -2.51596358e-03 -8.42613796e-03] 


########################################
Comparison with Test data
########################################


testing dataset  ####################
Shape of Feature matrix: (500, 100)
Shape of value vector: (500,)
Mean Square Error Loss computed on Test data  ####################
Maximum Likelihood Estimartor (Analytical): 0.37072731116979096
Maximum Likelihood Estimartor (Batch Gradient Descent): 0.37070214860750766
Maximum Likelihood Estimartor (Stochastic Gradient Descent w/- batch size of 100): 0.3580403098478923
Ridge Regression (Analytical): 0.36997126278713083
Ridge Regression (Batch Gradient Descent): 0.36994676373274016

